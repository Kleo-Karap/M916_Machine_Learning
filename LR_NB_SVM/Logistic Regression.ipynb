{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6cad7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "14e16a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "sex           int64\n",
       "cp            int64\n",
       "trestbps      int64\n",
       "chol          int64\n",
       "fbs           int64\n",
       "restecg       int64\n",
       "thalach       int64\n",
       "exang         int64\n",
       "oldpeak     float64\n",
       "slope         int64\n",
       "ca            int64\n",
       "thal          int64\n",
       "target        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=\"C:/Users/kleop/Documents/repos/Exercises/Machine_Learning/Coursework_2/heart.csv\"\n",
    "data=pd.read_csv(file_path, sep=',', decimal=\".\")\n",
    "#data.head()\n",
    "data.dtypes #to check. if there is any non-numeric variable, we should be omitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb4ea9",
   "metadata": {},
   "source": [
    "We want to check, whether all the above parameters(age, sex,trastbps...) have an effect on the probability of heart attack appearing on a person/patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe71cbe",
   "metadata": {},
   "source": [
    "Notes on features\n",
    "\n",
    "age(in years: discrete nominal),\n",
    "sex: 1-male/0-female (binary-categorical),\n",
    "cp: chest pain type (4 values: categorical),\n",
    "trestbps (discrete nominal),\n",
    "chol:discrete nominal,\n",
    "fbs: (fasting blood sugar: 1 = true; 0 = false) binary categorical,\n",
    "restecg: (3 values:0,1,2) categorical,\n",
    "thalach: maximum heart rate achieved (discrete nominal),\n",
    "exang: exercise induced angina (1 = yes; 0 = no) binary categorical,\n",
    "oldpeak: continuous nominal,\n",
    "slope: 3 values(0-upsloping,1-flat,2-downsloping): categorical,\n",
    "ca:number of major vessels(0-3) discerete ordinal,\n",
    "thal: 0 = normal; 1 = fixed defect; 2 = reversable defect (categorical)\n",
    "\n",
    "target:0(less chance of heartt attack), 1(more chance of heart attack) (binary categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ba0f49ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[63.  1.  3. ...  0.  0.  1.]\n",
      " [37.  1.  2. ...  0.  0.  2.]\n",
      " [41.  0.  1. ...  2.  0.  2.]\n",
      " ...\n",
      " [68.  1.  0. ...  1.  2.  3.]\n",
      " [57.  1.  0. ...  1.  1.  3.]\n",
      " [57.  0.  1. ...  1.  1.  2.]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Separate data into Matrix of features X and target variable y\n",
    "X=data.iloc[:,:-1].values\n",
    "y=data.iloc[:,-1].values\n",
    "\n",
    "print(X)\n",
    "print(y) #While printing the data, we get the intuition that the 2 classes (0 and 1) are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3a9c3",
   "metadata": {},
   "source": [
    "# Feature Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "76375c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9521966   0.68100522  1.97312292 ... -2.27457861 -0.71442887\n",
      "  -2.14887271]\n",
      " [-1.91531289  0.68100522  1.00257707 ... -2.27457861 -0.71442887\n",
      "  -0.51292188]\n",
      " [-1.47415758 -1.46841752  0.03203122 ...  0.97635214 -0.71442887\n",
      "  -0.51292188]\n",
      " ...\n",
      " [ 1.50364073  0.68100522 -0.93851463 ... -0.64911323  1.24459328\n",
      "   1.12302895]\n",
      " [ 0.29046364  0.68100522 -0.93851463 ... -0.64911323  0.26508221\n",
      "   1.12302895]\n",
      " [ 0.29046364 -1.46841752  0.03203122 ... -0.64911323  0.26508221\n",
      "  -0.51292188]]\n"
     ]
    }
   ],
   "source": [
    "scale=StandardScaler()\n",
    "X=scale.fit_transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa3dd9",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "951a9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719d57e",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) to reduce the dimensionality of the data in both Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ab690971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20879927 0.32995097 0.42369281 0.51514961 0.59630549 0.67101175\n",
      " 0.73647104 0.79594704 0.85092807 0.90117182]\n"
     ]
    }
   ],
   "source": [
    "# Principal Component Analysis to select k features such that they explain as much variance as possible\n",
    "pca =PCA(n_components =10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "#After trying with several number of variables starting from 2, I realize that for this specific dataset the dimensionality reduction\n",
    "#does not help with the exlainability of a significant amount of variance, in fact the 2 dimensions were covering nearly the 30%\n",
    "#of the variance of the dataset, so I chose to reduce the dimensions to 10, which does explain an over 90% of the variance of the\n",
    "#dataset.So I sacrifice the visualisation part for the sake of keeping as much important information as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "835a91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_expanded = poly.fit_transform(X_train_pca)\n",
    "X_test_expanded = poly.transform(X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5fd82",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a5575a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since the possible outcome is either 0 or 1,i.e we perform a binary classification task, we can perform binary logistic regression\n",
    "classifier=LogisticRegression(fit_intercept=True,max_iter=1000)\n",
    "classifier.fit(X_train_expanded,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7d132e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using the testing set\n",
    "y_pred_train=classifier.predict(X_train_expanded)\n",
    "y_pred_test=classifier.predict(X_test_expanded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587e136",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ff304f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of the Model:  0.9380165289256198\n",
      "Test Accuracy of the Model:  0.8360655737704918\n",
      "\n",
      "Confusion matrix for train: \n",
      " [[101  10]\n",
      " [  5 126]]\n",
      "Confusion matrix for test: \n",
      " [[21  6]\n",
      " [ 4 30]]\n",
      "\n",
      "Training Precision of the Model:  0.9264705882352942\n",
      "Test Precision of the Model:  0.8333333333333334\n",
      "\n",
      "Training Recall of the Model:  0.9618320610687023\n",
      "Test Recall of the Model:  0.8823529411764706\n",
      "\n",
      "Training F1-Score of the Model:  0.9438202247191011\n",
      "Test F1-Score of the Model:  0.8571428571428571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the Training and Test Accuracy of the Logistic Regression Model\n",
    "print('Training Accuracy of the Model: ', metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy of the Model: ', metrics.accuracy_score(y_test, y_pred_test))\n",
    "print()\n",
    "\n",
    "#Getting the confusion matrix for both training and test set\n",
    "print(\"Confusion matrix for train: \\n\",metrics.confusion_matrix(y_train, y_pred_train))\n",
    "print(\"Confusion matrix for test: \\n\",metrics.confusion_matrix(y_test,y_pred_test))\n",
    "print()\n",
    "\n",
    "# Getting the Training and Test Precision of the Logistic Regression Model\n",
    "print('Training Precision of the Model: ', metrics.precision_score(y_train, y_pred_train))\n",
    "print('Test Precision of the Model: ', metrics.precision_score(y_test, y_pred_test))\n",
    "print()\n",
    "\n",
    "# Getting the Training and Test Recall of the Logistic Regression Model\n",
    "print('Training Recall of the Model: ', metrics.recall_score(y_train, y_pred_train))\n",
    "print('Test Recall of the Model: ', metrics.recall_score(y_test, y_pred_test))\n",
    "print()\n",
    "\n",
    "# Getting the Training and Test F1-Score of the Logistic Regression Model\n",
    "print('Training F1-Score of the Model: ', metrics.f1_score(y_train, y_pred_train))\n",
    "print('Test F1-Score of the Model: ', metrics.f1_score(y_test, y_pred_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bddfad",
   "metadata": {},
   "source": [
    "# Regularisation using C=[1,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f58a00c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C=1: 0.8360655737704918\n",
      "Accuracy with C=5: 0.8524590163934426\n",
      "Accuracy with C=10: 0.8360655737704918\n"
     ]
    }
   ],
   "source": [
    "C_values = [1, 5, 10]\n",
    "\n",
    "# Train and evaluate logistic regression classifiers for each C value\n",
    "#solver=liblinear, because it is well-suited for binary classification problems.\n",
    "for C in C_values:\n",
    "    classifier = LogisticRegression(C=C, solver='liblinear', random_state=42)\n",
    "    classifier.fit(X_train_expanded, y_train)\n",
    "    y_pred = classifier.predict(X_test_expanded)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with C={C}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5590f",
   "metadata": {},
   "source": [
    "# Re-training the Logistic Regression Classifier with the best hyper-parameter, C = 5 (obtained above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7415bfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kleop\\anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# re-training the Logistic Regression Classifier with the best hyper-parameter, C = 5\n",
    "model = LogisticRegression(C = 5).fit(X_train_expanded, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3aac0",
   "metadata": {},
   "source": [
    "# Obtaining the Training Set and Test Set Predictions given by the model, trained in the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "520fbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the Training Set Predictions\n",
    "y_train_pred_lr = model.predict(X_train_expanded)\n",
    "\n",
    "# getting the Test Set Predictions\n",
    "y_test_pred_lr = model.predict(X_test_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004289c",
   "metadata": {},
   "source": [
    "# Performance Analysis of the Logistic Regression Model (with feature expansion) in terms of Accuracy and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3045ac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of the Model:  0.9669421487603306\n",
      "Test Accuracy of the Model:  0.8524590163934426\n",
      "\n",
      "Confusion matrix for train: \n",
      " [[106   5]\n",
      " [  3 128]]\n",
      "Confusion matrix for test: \n",
      " [[21  6]\n",
      " [ 3 31]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the Training and Test Accuracy of the Logistic Regression Model\n",
    "print('Training Accuracy of the Model: ', metrics.accuracy_score(y_train, y_train_pred_lr))\n",
    "print('Test Accuracy of the Model: ', metrics.accuracy_score(y_test, y_test_pred_lr))\n",
    "print()\n",
    "\n",
    "#Getting the confusion matrix for both training and test set\n",
    "print(\"Confusion matrix for train: \\n\",metrics.confusion_matrix(y_train, y_train_pred_lr))\n",
    "print(\"Confusion matrix for test: \\n\",metrics.confusion_matrix(y_test,y_test_pred_lr))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27eefed",
   "metadata": {},
   "source": [
    "The Logistic Regression model, after being trained with feature expansion methods, inicated the best performing results, by achieving an accuracy of over 85% on the test set! This must be due to the fact that the 'PolynomialFeatures' techinque  generates additional features, which capture nonlinear relationships or interactions between the existing features and such relationships could be hiding behind our data. After performing Pearson correlation, we observed that there is no siginificant  correlation between the variables, so feature expansion helped to find deeper relationships between our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff683f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
